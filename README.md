# my-chatgpt
Trying to understand how chatGPT's transformers works under the hood.

ChatGPT's design is based on a paper published in 2017 called [Attention is all you need.](https://arxiv.org/pdf/1706.03762.pdf). its  Transformer is based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.

# Limitations of GPTs
Despite their impressive capabilities, GPTs do have some limitations that it is important to be aware of:

1. GPTs are not able to generate completely original ideas or concepts. They can only generate text based on the information they have been trained on and the input provided.

2. The quality and coherence of the generated text depend on the specific GPT model and the quality of the training data used to train it. The generated text may be difficult to understand if the training data is of low quality or contains errors, the generated text may also be difficult to understand.

3. GPTs can be prone to biased output if the training data contains biased language or representations of certain groups. It is important to be aware of this and take steps to mitigate bias in the training data.

4. GPTs require many computational resources to train and use and can be expensive to implement at scale.

5. GPTs can only sometimes capture the subtleties of human language, and they may struggle with tasks that require a deeper understanding of context or meaning.
