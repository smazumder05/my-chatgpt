{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFdsjGnmhrBIRrRFn9CO7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smazumder05/my-chatgpt/blob/main/transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54mc3ESu3fzF"
      },
      "outputs": [],
      "source": [
        "!wget \"https://raw.githubusercontent.com/smazumder05/my-chatgpt/main/data/tiny-shakespear/input.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ead the file to inspect its contents and print out the number of characters\n",
        "with open(\"input.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(\"the number of charecters in the file:\", len(text))\n",
        "# print the first 2000 charecters in the file\n",
        "print(text[:2000])"
      ],
      "metadata": {
        "id": "SrXvY5lQ74Ap",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique characters in the file\n",
        "uniqueChars = sorted(list(set(text)))\n",
        "print(len(uniqueChars))\n",
        "print(''.join(uniqueChars))"
      ],
      "metadata": {
        "id": "gxaKq9uH-JSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic tokenizer, there are other tokenizer such as SentencePiece or tikTokenizer\n",
        "#create a mapping from charatecters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(uniqueChars)}\n",
        "itos = {i:ch for i, ch in enumerate(uniqueChars)}\n",
        "#encode takes a string and outputs a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "#decode takes a list of integers and outputs a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "id": "8SbIsHGXOOdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the tokenizer to encode the entire text from time Shakespear\n",
        "# using Pytorch library for torch data structure\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) #print the first 1000 characters"
      ],
      "metadata": {
        "id": "1mGVipjgQ-fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to train the model, we need to split the data into train and validation sets\n",
        "n = int(0.9*len(data)) #Get the first 90% of the data\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "VFPU8rwlSpwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with random text of maximum length also known as block size or context_size\n",
        "# we use a context_size of 8\n",
        "context_size = 8\n",
        "train_data[:context_size+1]\n",
        "#The transformer will learn to predict what char comes next, so it starts with 18 and predicts 47, if one gives it 18 47 it will predict 56 and so on"
      ],
      "metadata": {
        "id": "3Y-Kj4aNUwtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:context_size] #first context_size characters\n",
        "y = train_data[1:context_size+1] #next context_size + 1 characters, it is the target\n",
        "for t in range(context_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is: {context} the target is: {target}\")"
      ],
      "metadata": {
        "id": "6A5DHlh7WUXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now create batch dimension for efficiency\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 #how many will we process in parallel\n",
        "block_size = 8 # maximum context length for the predictions\n",
        "\n",
        "def get_batch(split):\n",
        "  #generate a small batch of data of x inputs and y targets\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('___________________________')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b :t+1]\n",
        "    target  = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target} \")\n",
        "    "
      ],
      "metadata": {
        "id": "u_QqqX05XsZ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}