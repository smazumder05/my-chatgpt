{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtUd0eUpqCdHhLXVHZprfZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smazumder05/my-chatgpt/blob/main/transform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54mc3ESu3fzF"
      },
      "outputs": [],
      "source": [
        "!wget \"https://raw.githubusercontent.com/smazumder05/my-chatgpt/main/data/tiny-shakespear/input.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file to inspect its contents and print out the number of characters\n",
        "with open(\"input.txt\", 'r', encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(\"the number of charecters in the file:\", len(text))\n",
        "# print the first 2000 charecters in the file\n",
        "print(text[:2000])"
      ],
      "metadata": {
        "id": "SrXvY5lQ74Ap",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique characters in the file\n",
        "uniqueChars = sorted(list(set(text)))\n",
        "vocab_size = len(uniqueChars)\n",
        "print(len(uniqueChars))\n",
        "print(''.join(uniqueChars))"
      ],
      "metadata": {
        "id": "gxaKq9uH-JSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic tokenizer, there are other tokenizer such as SentencePiece or tikTokenizer\n",
        "#create a mapping from charatecters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(uniqueChars)}\n",
        "itos = {i:ch for i, ch in enumerate(uniqueChars)}\n",
        "#encode takes a string and outputs a list of integers\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "#decode takes a list of integers and outputs a string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "id": "8SbIsHGXOOdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the tokenizer to encode the entire text from time Shakespear\n",
        "# using Pytorch library for torch data structure\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) #print the first 1000 characters"
      ],
      "metadata": {
        "id": "1mGVipjgQ-fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In order to train the model, we need to split the data into train and validation sets\n",
        "n = int(0.9*len(data)) #Get the first 90% of the data\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "VFPU8rwlSpwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model with random text of maximum length also known as block size or context_size\n",
        "# we use a context_size of 8\n",
        "context_size = 8\n",
        "train_data[:context_size+1]\n",
        "#The transformer will learn to predict what char comes next, so it starts with 18 and predicts 47, if one gives it 18 47 it will predict 56 and so on"
      ],
      "metadata": {
        "id": "3Y-Kj4aNUwtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:context_size] #first context_size characters\n",
        "y = train_data[1:context_size+1] #next context_size + 1 characters, it is the target\n",
        "for t in range(context_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print(f\"when input is: {context} the target is: {target}\")"
      ],
      "metadata": {
        "id": "6A5DHlh7WUXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now create batch dimension for efficiency\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 #how many will we process in parallel\n",
        "block_size = 8 # maximum context length for the predictions\n",
        "\n",
        "def get_batch(split):\n",
        "  #generate a small batch of data of x inputs and y targets\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "xb,yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('___________________________')\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context = xb[b :t+1]\n",
        "    target  = yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target} \")\n",
        "    "
      ],
      "metadata": {
        "id": "u_QqqX05XsZ6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) #print input to the transformer\n"
      ],
      "metadata": {
        "id": "fvYrdp7zmR2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self,idx,max_new_tokens):\n",
        "      # idx is (B,T) array of indices in the current context\n",
        "      for _ in range(max_new_tokens):\n",
        "        #get the predictions\n",
        "        logits,loss = self(idx)\n",
        "        #focus only on the last time step\n",
        "        logits = logits[:,-1, :]\n",
        "        #now apply softmax to get probablities\n",
        "        probs = F.softmax(logits,dim=-1)\n",
        "        #now sample the distribution\n",
        "        idx_next = torch.multinomial(probs,num_samples=1)\n",
        "        # append the sampled torchto the running sequence\n",
        "        idx = torch.cat((idx,idx_next), dim=1)\n",
        "      return idx\n",
        "   \n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "idx = torch.zeros((1,1),dtype=torch.long)\n",
        "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "nql_1ER53oCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets train the model using Pytorch optimizer AdamW optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
      ],
      "metadata": {
        "id": "sUOg8TkDuUpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(4000):\n",
        "  #sample a batch of data\n",
        "  xb,yb = get_batch('train')\n",
        "  #evaluate the loss\n",
        "  logits, loss = m(xb, yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b3T-Bw7RvMt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1,1),dtype=torch.long),max_new_tokens=1000)[0].tolist()))"
      ],
      "metadata": {
        "id": "DDPEbp9GwBb-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}